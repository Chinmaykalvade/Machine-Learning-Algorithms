{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web mining applications",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGZAfLKKc0Qs",
        "colab_type": "code",
        "outputId": "e4786a5f-4e89-47f3-87c0-c839d89e121f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ7YZ9wvYeYG",
        "colab_type": "text"
      },
      "source": [
        "##TF-IDF matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6YJv_R9T5lq",
        "colab_type": "code",
        "outputId": "13d412d2-25b3-4816-ff60-957678a5cf85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB as MB\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "stop_words = set(stopwords.words('english'))\n",
        "documents = []\n",
        "docsNo = 11\n",
        "def FreqCount(term, tokenize):\n",
        "  c=0\n",
        "  for f in tokenize:\n",
        "    if(term==f):\n",
        "      c=c+1\n",
        "  return c\n",
        "for i in range(1,docsNo+1):\n",
        "  documents.append(open(str(i)+'.txt','r').read())\n",
        "docs_tokens = []\n",
        "for i in range(1,docsNo+1):\n",
        "  docs_tokens.append(word_tokenize(documents[i-1]))\n",
        "tokenized = []\n",
        "t = []\n",
        "for i in range(1,docsNo+1):\n",
        "  t=[]\n",
        "  for w in docs_tokens[i-1]:\n",
        "    if w not in stop_words:\n",
        "      t.append(w)\n",
        "  tokenized.append(t)\n",
        "indexes = set(tokenized[0])\n",
        "for i in range(1,docsNo-1):\n",
        "  indexes.update(tokenized[i])\n",
        "df=pd.DataFrame(index=(range(1,docsNo+1)),columns=indexes)\n",
        "for each in indexes:\n",
        "  for i in range(0,docsNo):\n",
        "    df[each][i+1]=FreqCount(each, tokenized[i])\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Tuesday nearly became nation  ... Aeronautics performing needed Some\n",
            "1        0      0      0      0  ...           0          0      0    0\n",
            "2        0      1      0      0  ...           1          0      1    0\n",
            "3        0      0      0      1  ...           0          1      0    0\n",
            "4        2      0      0      0  ...           0          1      0    0\n",
            "5        1      0      0      0  ...           0          0      0    0\n",
            "6        0      0      0      0  ...           0          0      0    0\n",
            "7        0      0      0      1  ...           1          0      0    0\n",
            "8        1      0      1      0  ...           0          0      0    1\n",
            "9        0      0      0      0  ...           1          0      0    0\n",
            "10       0      0      0      0  ...           0          0      0    0\n",
            "11       0      0      0      0  ...           0          0      0    0\n",
            "\n",
            "[11 rows x 542 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5qAfsPd34bB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maxtokens = ['Rafale','government','The','Singh','India','jets','deal','fighter','Air','french']\n",
        "# columnsData = df.loc[ : , ['Rafale','government','The','Singh','India','jets','deal','fighter','Air','French'] ]\n",
        "# print(columnsData)\n",
        "df= df.values\n",
        "for j in range (0,11):\n",
        "  for i in range (0,11):\n",
        "    if(df[j][i]<3):\n",
        "      df[j][i]=0\n",
        "    if(3<=df[j][i]<5):\n",
        "      df[j][i]=1\n",
        "    if(5<=df[j][i]):\n",
        "      df[j][i]=2\n",
        "print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm_5j5B_0CXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sumdf= df.sum(axis = 0, skipna = True)\n",
        "# sumdf1=sumdf.sort_values(ascending=False)\n",
        "\n",
        "# # sumdf1 = sumdf.to_frame()\n",
        "\n",
        "# # sumdf2 = sumdf1.sort_values(by=['0'])\n",
        "\n",
        "# print(sumdf1)\n",
        "# sumdf= df.sum(axis = 0, skipna = True)\n",
        "# sumdf1 = sumdf.to_frame()\n",
        "\n",
        "# sumdf2 = sumdf1.sort_values(by=['0'])\n",
        "\n",
        "# print(sumdf1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvgHNJ_JHRe8",
        "colab_type": "code",
        "outputId": "5fabb768-230b-4192-d835-1323aa3ba2e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# maxtokens = ['Rafale','government','The','Singh','India','jets','deal','fighter','Air','french']\n",
        "col2 = pd.DataFrame(df, columns = indexes)\n",
        "print(col2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Tuesday nearly became nation  ... Aeronautics performing needed Some\n",
            "0        0      0      0      0  ...           0          0      0    0\n",
            "1        0      0      0      0  ...           1          0      1    0\n",
            "2        0      0      0      0  ...           0          1      0    0\n",
            "3        0      0      0      0  ...           0          1      0    0\n",
            "4        0      0      0      0  ...           0          0      0    0\n",
            "5        0      0      0      0  ...           0          0      0    0\n",
            "6        0      0      0      0  ...           1          0      0    0\n",
            "7        0      0      0      0  ...           0          0      0    1\n",
            "8        0      0      0      0  ...           1          0      0    0\n",
            "9        0      0      0      0  ...           0          0      0    0\n",
            "10       0      0      0      0  ...           0          0      0    0\n",
            "\n",
            "[11 rows x 542 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SVoMe3aT5oM",
        "colab_type": "code",
        "outputId": "f5c22808-a4aa-45de-a2de-2c728b69b7d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classifier =['2','1','2','2','2','1','0','1','0','0'] \n",
        "len(classifier)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRHyitMdT5qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB as MB\n",
        "trainingModel = MB(fit_prior=False,class_prior=[1/3,1/3,1/3])\n",
        "trainingModel.fit(col2.loc[0:11],classifier)\n",
        "\n",
        "MB(alpha=1.0, class_prior=[0.33, 0.33, 0.33], fit_prior=False)\n",
        "\n",
        "print(trainingModel.predict(np.array(df.loc[11,]).reshape(1,-1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0WS13GSYmj7",
        "colab_type": "text"
      },
      "source": [
        "##Page Rank algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6ltVjhRQL1E",
        "colab_type": "code",
        "outputId": "760d370f-9d7c-45d7-9531-f4c44ffa0d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#page rank\n",
        "def eigenvalue(A, v):\n",
        "    Av = A.dot(v)\n",
        "    return v.dot(Av)\n",
        "\n",
        "def power_iteration(A):# returns (new eigen value, new vector) \n",
        "    n, d = A.shape\n",
        "\n",
        "    v = np.ones(d) / np.sqrt(d)\n",
        "    ev = eigenvalue(A, v)\n",
        "\n",
        "    for i in range (0,30) :\n",
        "        Av = A.dot(v)\n",
        "        v_new = Av / np.linalg.norm(Av)\n",
        "\n",
        "        ev_new = eigenvalue(A, v_new)\n",
        "#         if np.abs(ev - ev_new) < 0.01:\n",
        "#             break\n",
        "\n",
        "        v = v_new\n",
        "        ev = ev_new\n",
        "\n",
        "    return ev_new, v_new\n",
        "  \n",
        "  \n",
        "import numpy as np  \n",
        "M = [[0,0,0.5,0],[1/3,0,0.5,1],[1/3,1,0,0],[1/3,0,0,0]] #define adjacenecy matrix as 2d array\n",
        "n = np.asarray(M)\n",
        "print(power_iteration(n))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.9998018065463701, array([0.35556579, 0.59286396, 0.71271537, 0.11883639]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgAwKUIwYySZ",
        "colab_type": "text"
      },
      "source": [
        "##Clustering of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO8ueRArF8F4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "''' This program takes a excel sheet as input where each row in first column of sheet represents a document.  '''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "data=pd.read_excel('data.xlsx') #Include your data file instead of data.xlsx\n",
        "idea=data.iloc[:,0:1] #Selecting the first column that has text.\n",
        "\n",
        "#Converting the column of data from excel sheet into a list of documents, where each document corresponds to a group of sentences.\n",
        "corpus=[]\n",
        "for index,row in idea.iterrows():\n",
        "    corpus.append(row['Idea'])\n",
        "\n",
        "'''Or you could just comment out the above code and use this dummy corpus list instead if you don't have the data.\n",
        "corpus=['She went to the airport to see him off.','I prefer reading to writing.','Los Angeles is in California. It's southeast of San Francisco.','I ate a burger then went to bed.','Compare your answer with Tom's.','I had hardly left home when it began to rain heavily.','If he had asked me, I would have given it to him. \n",
        "','I could have come by auto, but who would pay the fare? ','Whatever it may be, you should not have beaten him.','You should have told me yesterday','I should have joined this course last year.','Where are you going?','There are too many people here.','Everyone always asks me that.','I didn't think you were going to make it.','Be quiet while I am speaking.','I can't figure out why he said so.'] '''\n",
        "    \n",
        "    \n",
        "#Count Vectoriser then tidf transformer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "#vectorizer.get_feature_names()\n",
        "\n",
        "#print(X.toarray())     \n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(X)\n",
        "print(tfidf.shape )                        \n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "num_clusters = 5 #Change it according to your data.\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(tfidf)\n",
        "clusters = km.labels_.tolist()\n",
        "\n",
        "idea={'Idea':corpus, 'Cluster':clusters} #Creating dict having doc with the corresponding cluster number.\n",
        "frame=pd.DataFrame(idea,index=[clusters], columns=['Idea','Cluster']) # Converting it into a dataframe.\n",
        "\n",
        "print(\"\\n\")\n",
        "print(frame) #Print the doc with the labeled cluster number.\n",
        "print(\"\\n\")\n",
        "print(frame['Cluster'].value_counts()) #Print the counts of doc belonging to each cluster.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5aTM6E8tcEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in range(1,6):\n",
        "filename=str(p)+\".txt\"\n",
        "f1 = open(filename,\"r\")\n",
        "f2 = open(\"b.txt\",\"r\")\n",
        "f3 = open(\"c.txt\",\"w\")\n",
        "#stopword removal\n",
        "first_words=[]\n",
        "second_words=[]\n",
        "for line in f1:\n",
        "words = line.split()\n",
        "for w in words:\n",
        "first_words.append(w)\n",
        "for line in f2:\n",
        "w = line.split()\n",
        "for i in w:\n",
        "second_words.append(i)\n",
        "for word1 in first_words :\n",
        "for word2 in second_words:\n",
        "if word1==word2:\n",
        "first_words.remove(word2)\n",
        "for word in first_words:\n",
        "f3.write(word)\n",
        "f3.write(' ')\n",
        "print(str(p)+\"done\")\n",
        "f1.close()\n",
        "f2.close()\n",
        "f3.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgyMq2LoY_oH",
        "colab_type": "text"
      },
      "source": [
        "## Create TF-IDF matrix using web links and beautiful soup library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTqT7w5puL5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "# web_pages_files =\n",
        "# [\"https://en.m.wikipedia.org/w/index.php?title=Data_mining&oldid=917016037\",\"h\n",
        "# ttps://en.m.wikipedia.org/w/index.php?title=Web_mining&oldid=915172323\",\"https\n",
        "# ://en.m.wikipedia.org/w/index.php?title=Web_analytics&oldid=914565724\"]\n",
        "# from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize,RegexpTokenizer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from collections import Counter\n",
        "import operator as o\n",
        "import math as m\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = stopwords.words('english')\n",
        "lancaster = LancasterStemmer()\n",
        "z=[]\n",
        "for page in web_pages_files:\n",
        "  a = []\n",
        "  b = []\n",
        "  c = []\n",
        "  html = urllib.request.urlopen(page).read()\n",
        "  soup = BeautifulSoup(html)\n",
        "  text = soup.get_text()\n",
        "  text = text.lower()\n",
        " #a is after removing punctuation\n",
        "  a = tokenizer.tokenize(text)\n",
        "\n",
        " #b is after removing stopwords\n",
        " for word in a:\n",
        " if word not in stop_words: #Removing stopwords\n",
        " b.append(word)\n",
        "\n",
        " #c is after stemming\n",
        " for word in b:\n",
        " c.append(lancaster.stem(word))\n",
        " #new_text=\"\"\n",
        " #for word in c:\n",
        " # new_text=new_text+\" \"+word\n",
        " z.append(c)\n",
        "\n",
        "#print(z)\n",
        "print(\"--------------------------------------------\")\n",
        "total_words = []\n",
        "p=[]\n",
        "tf=[[],[],[]]\n",
        "for i in z:\n",
        " total_words.append(len(i))\n",
        "#print(total_words)\n",
        "l=0\n",
        "sq=[]\n",
        "for page_term in z:\n",
        " total=0\n",
        " term_count = Counter(page_term)\n",
        " #print(term_count)\n",
        " p.append(term_count)\n",
        "for i in p:\n",
        " sq.append(0)\n",
        " for x in i:\n",
        " i[x] = i[x]/total_words[l]\n",
        " sq[l] += pow(i[x],2)\n",
        " sq[l]=pow(sq[l],0.5)\n",
        " for x in i:\n",
        " i[x] = i[x]/sq[l]\n",
        " l=l+1\n",
        "#print(p)\n",
        "tf_norm = p.copy()\n",
        "idfset = set()\n",
        "for i in z:\n",
        " for x in i:\n",
        " idfset.add(x)\n",
        "idf = {}\n",
        "sq = 0\n",
        "for i in idfset:\n",
        " doc=0\n",
        " for j in z:\n",
        " if i in j:\n",
        " doc+=1\n",
        " idf[i]=m.log(4/doc)\n",
        " sq = pow(idf[i],2.00000)\n",
        "sq = pow(sq,0.5000000)\n",
        "for i in idf:\n",
        " idf[i]=idf[i]/sq\n",
        "idf_norm = idf.copy()\n",
        "terms = []\n",
        "for i in range(20):\n",
        "  temp = idf_norm.copy()\n",
        "  for x in terms:\n",
        "  temp[x] = 0\n",
        "  terms.append(max(temp, key=temp.get))\n",
        "tfidf = [{},{},{}]\n",
        "for j in range(3):\n",
        " for i in terms:\n",
        " tfidf[j][i]=tf_norm[j][i]*idf_norm[i]\n",
        "print(\"\\t\",end=\"\\t\")\n",
        "for i in terms:\n",
        " print(i,end=\"\\t\")\n",
        "print()\n",
        "for i in range(3):\n",
        " print(i,end=\"\\t\")\n",
        " for j in tfidf[i]:\n",
        " print(tfidf[i][j],end=\"\\t\")\n",
        " print()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}